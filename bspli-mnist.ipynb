{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist data shape: (60000, 784)\n",
      "mnist dtype: float32\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./Bspli/\")\n",
    "import os  \n",
    "import faiss\n",
    "import time\n",
    "import numpy as np \n",
    "import index \n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "mnist = np.load(\"dataset/mnist-784-euclidean.npy\")\n",
    "print(f'mnist data shape: {mnist.shape}')\n",
    "print(f'mnist dtype: {mnist.dtype}')\n",
    "\n",
    "# mnist = np.random.rand(60000, 2)\n",
    "# mnist = mnist.astype(np.float32)\n",
    "# print(f'mnist dtype: {mnist.dtype}')\n",
    "# print(f'mnist data shape: {mnist.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brute query: [[    0 32248  8728 18932 30483 24149 42338 52295 26251 50173 53634 24330\n",
      "  54159 57528  1482 53428 18123 31379 52864 10536 29719 36087 30489 23947\n",
      "  20034 52057 33825 21654 31008 55208 22477 44282 47968 54203 19825  1634\n",
      "  27378 33909 15378 24708 34474 26413 16017 46824 46358  1516 34557 16832\n",
      "  21629 29021 10740 24107  5688 52665  1864  5036 39031  1978 40546 22322\n",
      "  52231 37284 24730  5970 21976 16945  9568 36697 25675 54189 11396 42555\n",
      "  33445 52540 44263 18404 19186 24232 54184 25762 14736 33970  5210 59212\n",
      "   8642 22569 15052  2933  6772 22963  6516   832 21244 21583 35838 59846\n",
      "  21210 13502 52559 13862]]\n",
      "CPU times: total: 93.8 ms\n",
      "Wall time: 98.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Brute Search\n",
    "flat = faiss.IndexFlatL2(mnist.shape[1])\n",
    "flat.add(mnist)\n",
    "\n",
    "D, FLAT_I = flat.search(mnist[0].reshape(1, mnist.shape[1]), k=100) \n",
    "print(f'brute query: {FLAT_I}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist tensor shape: torch.Size([60000, 784])\n",
      "torch.Size([60000, 785])\n",
      "first stage partitioning finish\n",
      "partitioning blocks : 1\n",
      "training local model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\loss.py:988: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.huber_loss(input, target, reduction=self.reduction, delta=self.delta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 100 loss: 0.2698622512817383\n",
      "1, 200 loss: 0.2705184745788574\n",
      "1, 300 loss: 0.2413215446472168\n",
      "1, 400 loss: 0.24074575424194336\n",
      "1, 500 loss: 0.19300430297851562\n",
      "1, 600 loss: 0.19833263397216797\n",
      "2, 100 loss: 0.17897863388061525\n",
      "2, 200 loss: 0.17321250915527345\n",
      "2, 300 loss: 0.17073883056640626\n",
      "2, 400 loss: 0.15729809761047364\n",
      "2, 500 loss: 0.15273608207702638\n",
      "2, 600 loss: 0.15807568550109863\n",
      "3, 100 loss: 0.1550144386291504\n",
      "3, 200 loss: 0.13744481086730956\n",
      "3, 300 loss: 0.15629261016845702\n",
      "3, 400 loss: 0.15460164070129395\n",
      "3, 500 loss: 0.15858479499816894\n",
      "3, 600 loss: 0.15264425277709961\n",
      "4, 100 loss: 0.14636314392089844\n",
      "4, 200 loss: 0.1532992935180664\n",
      "4, 300 loss: 0.15706639289855956\n",
      "4, 400 loss: 0.16868106842041017\n",
      "4, 500 loss: 0.1564497947692871\n",
      "4, 600 loss: 0.15770397186279297\n",
      "5, 100 loss: 0.13963208198547364\n",
      "5, 200 loss: 0.15714559555053711\n",
      "5, 300 loss: 0.14544346809387207\n",
      "5, 400 loss: 0.14833748817443848\n",
      "5, 500 loss: 0.1663178825378418\n",
      "5, 600 loss: 0.15412880897521972\n",
      "6, 100 loss: 0.14979790687561034\n",
      "6, 200 loss: 0.14908572196960448\n",
      "6, 300 loss: 0.14017865180969238\n",
      "6, 400 loss: 0.1682724380493164\n",
      "6, 500 loss: 0.15945398330688476\n",
      "6, 600 loss: 0.15990633964538575\n",
      "7, 100 loss: 0.13675698280334472\n",
      "7, 200 loss: 0.15582724571228027\n",
      "7, 300 loss: 0.15921600341796874\n",
      "7, 400 loss: 0.14617682456970216\n",
      "7, 500 loss: 0.13112350463867187\n",
      "7, 600 loss: 0.14113673210144043\n",
      "8, 100 loss: 0.15378270149230958\n",
      "8, 200 loss: 0.1488479995727539\n",
      "8, 300 loss: 0.1474546241760254\n",
      "8, 400 loss: 0.15184359550476073\n",
      "8, 500 loss: 0.1370442771911621\n",
      "8, 600 loss: 0.1389082622528076\n",
      "9, 100 loss: 0.15196800231933594\n",
      "9, 200 loss: 0.16163305282592774\n",
      "9, 300 loss: 0.1523126220703125\n",
      "9, 400 loss: 0.14780165672302245\n",
      "9, 500 loss: 0.1473117923736572\n",
      "9, 600 loss: 0.1536453151702881\n",
      "10, 100 loss: 0.14262269020080567\n",
      "10, 200 loss: 0.1440164852142334\n",
      "10, 300 loss: 0.17614479064941407\n",
      "10, 400 loss: 0.13857762336730958\n",
      "10, 500 loss: 0.1644746780395508\n",
      "10, 600 loss: 0.16079067230224608\n",
      "11, 100 loss: 0.15589753150939942\n",
      "11, 200 loss: 0.15883923530578614\n",
      "11, 300 loss: 0.1545201587677002\n",
      "11, 400 loss: 0.15235877990722657\n",
      "11, 500 loss: 0.15294718742370605\n",
      "11, 600 loss: 0.15592262268066406\n",
      "12, 100 loss: 0.15123993873596192\n",
      "12, 200 loss: 0.13308393478393554\n",
      "12, 300 loss: 0.15143725395202637\n",
      "12, 400 loss: 0.15040641784667969\n",
      "12, 500 loss: 0.1565435314178467\n",
      "12, 600 loss: 0.1378865909576416\n",
      "13, 100 loss: 0.14951818466186523\n",
      "13, 200 loss: 0.14794532775878907\n",
      "13, 300 loss: 0.16759986877441407\n",
      "13, 400 loss: 0.1437519359588623\n",
      "13, 500 loss: 0.1516884994506836\n",
      "13, 600 loss: 0.14147125244140624\n",
      "14, 100 loss: 0.15161157608032227\n",
      "14, 200 loss: 0.15243974685668946\n",
      "14, 300 loss: 0.14717719078063965\n",
      "14, 400 loss: 0.15648115158081055\n",
      "14, 500 loss: 0.1586492156982422\n",
      "14, 600 loss: 0.1614857482910156\n",
      "15, 100 loss: 0.1523292064666748\n",
      "15, 200 loss: 0.15971219062805175\n",
      "15, 300 loss: 0.14248779296875\n",
      "15, 400 loss: 0.1673997688293457\n",
      "15, 500 loss: 0.15447037696838378\n",
      "15, 600 loss: 0.1649653434753418\n",
      "16, 100 loss: 0.15353777885437012\n",
      "16, 200 loss: 0.15115114212036132\n",
      "16, 300 loss: 0.15174159049987793\n",
      "16, 400 loss: 0.1421392345428467\n",
      "16, 500 loss: 0.14256972312927246\n",
      "16, 600 loss: 0.1513417625427246\n",
      "17, 100 loss: 0.14607903480529785\n",
      "17, 200 loss: 0.15602622032165528\n",
      "17, 300 loss: 0.1587839889526367\n",
      "17, 400 loss: 0.14924915313720702\n",
      "17, 500 loss: 0.1603069496154785\n",
      "17, 600 loss: 0.15050768852233887\n",
      "18, 100 loss: 0.15894187927246095\n",
      "18, 200 loss: 0.1440977954864502\n",
      "18, 300 loss: 0.1417922019958496\n",
      "18, 400 loss: 0.15073731422424316\n",
      "18, 500 loss: 0.14251673698425293\n",
      "18, 600 loss: 0.16460559844970704\n",
      "19, 100 loss: 0.13551462173461915\n",
      "19, 200 loss: 0.14365094184875488\n",
      "19, 300 loss: 0.14096118927001952\n",
      "19, 400 loss: 0.14376449584960938\n",
      "19, 500 loss: 0.1526852798461914\n",
      "19, 600 loss: 0.139398250579834\n",
      "20, 100 loss: 0.14916462898254396\n",
      "20, 200 loss: 0.15412932395935058\n",
      "20, 300 loss: 0.15765703201293946\n",
      "20, 400 loss: 0.13531962394714356\n",
      "20, 500 loss: 0.15598076820373535\n",
      "20, 600 loss: 0.13119010925292968\n",
      "21, 100 loss: 0.14940393447875977\n",
      "21, 200 loss: 0.1491386604309082\n",
      "21, 300 loss: 0.13549647331237794\n",
      "21, 400 loss: 0.14691865921020508\n",
      "21, 500 loss: 0.15384871482849122\n",
      "21, 600 loss: 0.148530797958374\n",
      "22, 100 loss: 0.15343992233276368\n",
      "22, 200 loss: 0.13994417190551758\n",
      "22, 300 loss: 0.16254480361938475\n",
      "22, 400 loss: 0.14918399810791017\n",
      "22, 500 loss: 0.1359852695465088\n",
      "22, 600 loss: 0.1476065158843994\n",
      "23, 100 loss: 0.14239815711975098\n",
      "23, 200 loss: 0.14112821578979493\n",
      "23, 300 loss: 0.1516384983062744\n",
      "23, 400 loss: 0.13154717445373534\n",
      "23, 500 loss: 0.148486967086792\n",
      "23, 600 loss: 0.15628798484802245\n",
      "24, 100 loss: 0.14317445755004882\n",
      "24, 200 loss: 0.14597023010253907\n",
      "24, 300 loss: 0.1338120937347412\n",
      "24, 400 loss: 0.151549768447876\n",
      "24, 500 loss: 0.15014714241027832\n",
      "24, 600 loss: 0.16067096710205078\n",
      "25, 100 loss: 0.15170881271362305\n",
      "25, 200 loss: 0.15600000381469725\n",
      "25, 300 loss: 0.1657903289794922\n",
      "25, 400 loss: 0.1575658893585205\n",
      "25, 500 loss: 0.1342024040222168\n",
      "25, 600 loss: 0.1427519989013672\n",
      "26, 100 loss: 0.15852176666259765\n",
      "26, 200 loss: 0.15691645622253417\n",
      "26, 300 loss: 0.14842870712280273\n",
      "26, 400 loss: 0.15296465873718262\n",
      "26, 500 loss: 0.16456287384033202\n",
      "26, 600 loss: 0.1440022850036621\n",
      "27, 100 loss: 0.1510562801361084\n",
      "27, 200 loss: 0.17843305587768554\n",
      "27, 300 loss: 0.1483975315093994\n",
      "27, 400 loss: 0.15827202796936035\n",
      "27, 500 loss: 0.15272676467895507\n",
      "27, 600 loss: 0.14850504875183104\n",
      "28, 100 loss: 0.16358619689941406\n",
      "28, 200 loss: 0.14807015419006347\n",
      "28, 300 loss: 0.1482091426849365\n",
      "28, 400 loss: 0.13286429405212402\n",
      "28, 500 loss: 0.1568957805633545\n",
      "28, 600 loss: 0.15832887649536131\n",
      "29, 100 loss: 0.1577271842956543\n",
      "29, 200 loss: 0.14105892181396484\n",
      "29, 300 loss: 0.15139198303222656\n",
      "29, 400 loss: 0.13964080810546875\n",
      "29, 500 loss: 0.15647130966186523\n",
      "29, 600 loss: 0.13909911155700683\n",
      "30, 100 loss: 0.15538117408752442\n",
      "30, 200 loss: 0.15153846740722657\n",
      "30, 300 loss: 0.1528799819946289\n",
      "30, 400 loss: 0.1679837417602539\n",
      "30, 500 loss: 0.14532943725585937\n",
      "30, 600 loss: 0.14670937538146972\n",
      "trainging global model\n",
      "global index train smaple count: 125\n",
      "finish\n",
      "local model len:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\bspli\\./Bspli\\utils\\Partitioning.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ma = torch.tensor(means)\n",
      "d:\\conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\loss.py:928: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    }
   ],
   "source": [
    "mnist_tensor = torch.from_numpy(mnist)\n",
    "print(f'mnist tensor shape: {mnist_tensor.shape}')\n",
    "\n",
    "idx = index.Indexing(\n",
    "    gl_size=80000, \n",
    "    ll_size=1000,\n",
    "    g_epoch_num=5,\n",
    "    l_epoch_num=30,\n",
    "    g_hidden_size=5,\n",
    "    l_hidden_size=10,\n",
    "    l_block_range=25,\n",
    "    random_partitioning=False\n",
    ")\n",
    "idx.train(mnist_tensor)\n",
    "\n",
    "print(f\"local model len:{len(idx._l_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.48\n",
      "pred: tensor([    0,  8728, 24149, 42338, 52295, 50173, 53634, 24330,  1482, 18123,\n",
      "        23947, 20034, 52057, 44282, 54203, 19825, 33909, 24708, 46824,  1516,\n",
      "        34557, 29021, 10740, 24107, 52665,  1864,  5036, 40546, 22322, 21976,\n",
      "        36697, 25675, 54189, 11396, 42555, 52540, 44263, 25762, 14736,  5210,\n",
      "        59212, 22569, 21244, 35838, 21210, 13502, 52559, 13862, 41980, 43997,\n",
      "        53812, 15975, 50187, 28384, 24271, 21661, 15907,  4622, 50301, 38896,\n",
      "        15444, 50071, 45567, 26313,  2395, 11936, 16112, 49334, 44476, 18189,\n",
      "        17298, 26364, 36242, 18197, 25557,  8230, 29649, 21543, 45432,  3005,\n",
      "        14543, 15415, 40432, 22542, 33607, 43067,  9652, 21563, 25623, 54283,\n",
      "        52310, 42971, 24281, 20570, 54131,  2037, 25720, 36695, 32001,  6274],\n",
      "       dtype=torch.int32)\n",
      "CPU times: total: 359 ms\n",
      "Wall time: 56.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def recall(pred, true):\n",
    "    x = np.isin(pred, true)\n",
    "    return x.sum() / true.size\n",
    "\n",
    "qp = torch.from_numpy(mnist[0])\n",
    "# print(qp)\n",
    "pred = idx.query(qp, k=100)\n",
    "print(f\"recall: {recall(pred, FLAT_I)}\")\n",
    "\n",
    "pred = pred.to(torch.int)\n",
    "print(f\"pred: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "def benchmark_knn_query(data, index, size=1000, k=100):\n",
    "    indices = np.random.choice(data.shape[0], size, replace=False)\n",
    "    query_time = 0\n",
    "    cur_recall = 0\n",
    "\n",
    "    # query\n",
    "    for i in indices:\n",
    "        q = torch.from_numpy(data[i])\n",
    "        start = time.time()\n",
    "        qk = index.query(q, k=100)\n",
    "        query_time += (time.time() - start)\n",
    "        D, FLAT_I = flat.search(data[i].reshape(1, data.shape[1]), k=k) \n",
    "        cur_recall += recall(qk, FLAT_I)\n",
    "    result.append((query_time/1000, cur_recall/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall(pred, FLAT_I)\n",
    "\n",
    "benchmark_knn_query(mnist, idx, size=1000, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.047600539445877076, 0.42035999999999946)]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
