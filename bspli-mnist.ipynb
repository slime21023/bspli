{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist data shape: (60000, 784)\n",
      "mnist dtype: float32\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./Bspli/\")\n",
    "import os  \n",
    "import faiss\n",
    "import time\n",
    "import numpy as np \n",
    "import index \n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "mnist = np.load(\"dataset/mnist-784-euclidean.npy\")\n",
    "print(f'mnist data shape: {mnist.shape}')\n",
    "print(f'mnist dtype: {mnist.dtype}')\n",
    "\n",
    "# mnist = np.random.uniform(low=0, high=255, size=(100000, 2))\n",
    "# mnist = mnist.astype(np.float32)\n",
    "# print(f'mnist dtype: {mnist.dtype}')\n",
    "# print(f'mnist data shape: {mnist.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brute query: [[    0 32248  8728 18932 30483 24149 42338 52295 26251 50173 53634 24330\n",
      "  54159 57528  1482 53428 18123 31379 52864 10536 29719 36087 30489 23947\n",
      "  20034 52057 33825 21654 31008 55208 22477 44282 47968 54203 19825  1634\n",
      "  27378 33909 15378 24708 34474 26413 16017 46824 46358  1516 34557 16832\n",
      "  21629 29021 10740 24107  5688 52665  1864  5036 39031  1978 40546 22322\n",
      "  52231 37284 24730  5970 21976 16945  9568 36697 25675 54189 11396 42555\n",
      "  33445 52540 44263 18404 19186 24232 54184 25762 14736 33970  5210 59212\n",
      "   8642 22569 15052  2933  6772 22963  6516   832 21244 21583 35838 59846\n",
      "  21210 13502 52559 13862]]\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 78.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Brute Search\n",
    "flat = faiss.IndexFlatL2(mnist.shape[1])\n",
    "flat.add(mnist)\n",
    "\n",
    "D, FLAT_I = flat.search(mnist[0].reshape(1, mnist.shape[1]), k=100) \n",
    "print(f'brute query: {FLAT_I}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist tensor shape: torch.Size([60000, 784])\n",
      "torch.Size([1995, 785])\n",
      "torch.Size([8906, 785])\n",
      "torch.Size([4610, 785])\n",
      "torch.Size([3738, 785])\n",
      "torch.Size([7329, 785])\n",
      "torch.Size([4915, 785])\n",
      "torch.Size([8447, 785])\n",
      "torch.Size([4660, 785])\n",
      "torch.Size([2740, 785])\n",
      "torch.Size([2620, 785])\n",
      "torch.Size([382, 785])\n",
      "torch.Size([9658, 785])\n",
      "first stage partitioning finish\n",
      "partitioning blocks : 12\n",
      "training local model\n",
      "1, 10 loss: -0.0\n",
      "2, 10 loss: -0.0\n",
      "3, 10 loss: -0.0\n",
      "4, 10 loss: -0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5, 10 loss: -0.0\n",
      "6, 10 loss: -0.0\n",
      "7, 10 loss: -0.0\n",
      "8, 10 loss: -0.0\n",
      "9, 10 loss: -0.0\n",
      "10, 10 loss: -0.0\n",
      "training local model\n",
      "1, 10 loss: 0.008314797878265381\n",
      "1, 20 loss: 0.007823784351348877\n",
      "1, 30 loss: 0.00731235682964325\n",
      "1, 40 loss: 0.007242535948753357\n",
      "2, 10 loss: 0.006758679747581482\n",
      "2, 20 loss: 0.006528628468513489\n",
      "2, 30 loss: 0.006562437415122985\n",
      "2, 40 loss: 0.0066947627067565916\n",
      "3, 10 loss: 0.006260637044906616\n",
      "3, 20 loss: 0.006269686818122864\n",
      "3, 30 loss: 0.006140231490135193\n",
      "3, 40 loss: 0.006330084204673767\n",
      "4, 10 loss: 0.0061553555727005\n",
      "4, 20 loss: 0.006210472583770752\n",
      "4, 30 loss: 0.0061468368768692015\n",
      "4, 40 loss: 0.006129461526870728\n",
      "5, 10 loss: 0.006028050184249878\n",
      "5, 20 loss: 0.005976652503013611\n",
      "5, 30 loss: 0.006014922261238098\n",
      "5, 40 loss: 0.005825188159942627\n",
      "6, 10 loss: 0.0058271831274032595\n",
      "6, 20 loss: 0.0059806954860687255\n",
      "6, 30 loss: 0.005940693020820617\n",
      "6, 40 loss: 0.005667051672935486\n",
      "7, 10 loss: 0.005983898043632507\n",
      "7, 20 loss: 0.005835117101669312\n",
      "7, 30 loss: 0.005771670937538147\n",
      "7, 40 loss: 0.00589323878288269\n",
      "8, 10 loss: 0.005832173824310303\n",
      "8, 20 loss: 0.0057793563604354855\n",
      "8, 30 loss: 0.0058383017778396605\n",
      "8, 40 loss: 0.00605324387550354\n",
      "9, 10 loss: 0.005743938088417053\n",
      "9, 20 loss: 0.0057555299997329714\n",
      "9, 30 loss: 0.005961188673973083\n",
      "9, 40 loss: 0.005950698256492614\n",
      "10, 10 loss: 0.005733503103256226\n",
      "10, 20 loss: 0.005715543627738953\n",
      "10, 30 loss: 0.005780174136161804\n",
      "10, 40 loss: 0.0057741540670394895\n",
      "training local model\n",
      "1, 10 loss: 0.0022442272305488588\n",
      "1, 20 loss: 0.0022647467255592344\n",
      "2, 10 loss: 0.002025361508131027\n",
      "2, 20 loss: 0.0020298820734024046\n",
      "3, 10 loss: 0.002024729549884796\n",
      "3, 20 loss: 0.002239377647638321\n",
      "4, 10 loss: 0.00203929141163826\n",
      "4, 20 loss: 0.002107749581336975\n",
      "5, 10 loss: 0.0019475342333316803\n",
      "5, 20 loss: 0.0019732823967933653\n",
      "6, 10 loss: 0.0019235317409038544\n",
      "6, 20 loss: 0.001975308358669281\n",
      "7, 10 loss: 0.00200266495347023\n",
      "7, 20 loss: 0.001942334622144699\n",
      "8, 10 loss: 0.0020303775370121\n",
      "8, 20 loss: 0.001954150050878525\n",
      "9, 10 loss: 0.0019424736499786377\n",
      "9, 20 loss: 0.001924572139978409\n",
      "10, 10 loss: 0.0019282212853431702\n",
      "10, 20 loss: 0.0020101098716259003\n",
      "training local model\n",
      "1, 10 loss: 0.002437825053930283\n",
      "2, 10 loss: 0.0020628608763217926\n",
      "3, 10 loss: 0.0020375889539718628\n",
      "4, 10 loss: 0.0020455342531204225\n",
      "5, 10 loss: 0.001983327120542526\n",
      "6, 10 loss: 0.0020391036570072173\n",
      "7, 10 loss: 0.0020015811920166015\n",
      "8, 10 loss: 0.001912558376789093\n",
      "9, 10 loss: 0.001974674612283707\n",
      "10, 10 loss: 0.002098834365606308\n",
      "training local model\n",
      "1, 10 loss: 0.007700111269950866\n",
      "1, 20 loss: 0.007213494777679443\n",
      "1, 30 loss: 0.006905806660652161\n",
      "2, 10 loss: 0.006589118242263794\n",
      "2, 20 loss: 0.006315486431121826\n",
      "2, 30 loss: 0.006483355760574341\n",
      "3, 10 loss: 0.006048130989074707\n",
      "3, 20 loss: 0.006293184161186218\n",
      "3, 30 loss: 0.005992830395698547\n",
      "4, 10 loss: 0.005806240439414978\n",
      "4, 20 loss: 0.005948167443275452\n",
      "4, 30 loss: 0.00579164981842041\n",
      "5, 10 loss: 0.005953010320663452\n",
      "5, 20 loss: 0.005747370719909668\n",
      "5, 30 loss: 0.005833224058151245\n",
      "6, 10 loss: 0.005723661184310913\n",
      "6, 20 loss: 0.005588375329971314\n",
      "6, 30 loss: 0.005589181780815125\n",
      "7, 10 loss: 0.005551476478576661\n",
      "7, 20 loss: 0.005549729466438293\n",
      "7, 30 loss: 0.005606093406677246\n",
      "8, 10 loss: 0.0055741238594055175\n",
      "8, 20 loss: 0.005546079277992249\n",
      "8, 30 loss: 0.005597542524337768\n",
      "9, 10 loss: 0.005462858080863953\n",
      "9, 20 loss: 0.00563016951084137\n",
      "9, 30 loss: 0.00569773018360138\n",
      "10, 10 loss: 0.0054462277889251705\n",
      "10, 20 loss: 0.0055042123794555664\n",
      "10, 30 loss: 0.0054764246940612795\n",
      "training local model\n",
      "1, 10 loss: 0.004317063093185425\n",
      "1, 20 loss: 0.0037801185250282286\n",
      "2, 10 loss: 0.003488250970840454\n",
      "2, 20 loss: 0.003634118139743805\n",
      "3, 10 loss: 0.003463340401649475\n",
      "3, 20 loss: 0.003413621187210083\n",
      "4, 10 loss: 0.003554539978504181\n",
      "4, 20 loss: 0.0033124309778213503\n",
      "5, 10 loss: 0.0032035431265830996\n",
      "5, 20 loss: 0.003163594603538513\n",
      "6, 10 loss: 0.003346981406211853\n",
      "6, 20 loss: 0.0033148258924484255\n",
      "7, 10 loss: 0.0031951704621315004\n",
      "7, 20 loss: 0.0033310669660568237\n",
      "8, 10 loss: 0.0031711527705192565\n",
      "8, 20 loss: 0.0033094993233680724\n",
      "9, 10 loss: 0.003140992820262909\n",
      "9, 20 loss: 0.003184829354286194\n",
      "10, 10 loss: 0.0031459492444992064\n",
      "10, 20 loss: 0.0031227627396583557\n",
      "training local model\n",
      "1, 10 loss: 0.008215492367744446\n",
      "1, 20 loss: 0.00734771192073822\n",
      "1, 30 loss: 0.0071006065607070925\n",
      "1, 40 loss: 0.006926674842834473\n",
      "2, 10 loss: 0.006412119865417481\n",
      "2, 20 loss: 0.006421546936035156\n",
      "2, 30 loss: 0.006271909475326538\n",
      "2, 40 loss: 0.006319587230682373\n",
      "3, 10 loss: 0.00609042763710022\n",
      "3, 20 loss: 0.005973995327949524\n",
      "3, 30 loss: 0.006115081906318665\n",
      "3, 40 loss: 0.00613624095916748\n",
      "4, 10 loss: 0.006028888821601868\n",
      "4, 20 loss: 0.006253721714019775\n",
      "4, 30 loss: 0.006071058511734009\n",
      "4, 40 loss: 0.006001831293106079\n",
      "5, 10 loss: 0.005738987922668457\n",
      "5, 20 loss: 0.005824371576309204\n",
      "5, 30 loss: 0.0057944577932357785\n",
      "5, 40 loss: 0.005859538912773132\n",
      "6, 10 loss: 0.00590451717376709\n",
      "6, 20 loss: 0.006012772917747497\n",
      "6, 30 loss: 0.0057453858852386476\n",
      "6, 40 loss: 0.005779672861099244\n",
      "7, 10 loss: 0.005573493838310242\n",
      "7, 20 loss: 0.005836166143417358\n",
      "7, 30 loss: 0.0057340991497039796\n",
      "7, 40 loss: 0.005619789361953735\n",
      "8, 10 loss: 0.005810699462890625\n",
      "8, 20 loss: 0.005761678218841553\n",
      "8, 30 loss: 0.005537304878234863\n",
      "8, 40 loss: 0.00576730489730835\n",
      "9, 10 loss: 0.005781697034835815\n",
      "9, 20 loss: 0.005532782077789306\n",
      "9, 30 loss: 0.0056633341312408445\n",
      "9, 40 loss: 0.005703698992729187\n",
      "10, 10 loss: 0.005628545880317688\n",
      "10, 20 loss: 0.005708155035972595\n",
      "10, 30 loss: 0.005719482302665711\n",
      "10, 40 loss: 0.005493631958961487\n",
      "training local model\n",
      "1, 10 loss: 0.0055972284078598025\n",
      "1, 20 loss: 0.005067436695098877\n",
      "2, 10 loss: 0.0046511352062225345\n",
      "2, 20 loss: 0.004480754733085632\n",
      "3, 10 loss: 0.004383828043937683\n",
      "3, 20 loss: 0.004233625829219818\n",
      "4, 10 loss: 0.004221810400485993\n",
      "4, 20 loss: 0.00441658467054367\n",
      "5, 10 loss: 0.004087556600570679\n",
      "5, 20 loss: 0.004234507083892823\n",
      "6, 10 loss: 0.00418318510055542\n",
      "6, 20 loss: 0.004346511960029602\n",
      "7, 10 loss: 0.004136993288993835\n",
      "7, 20 loss: 0.004236642718315125\n",
      "8, 10 loss: 0.004187957644462586\n",
      "8, 20 loss: 0.004101857244968414\n",
      "9, 10 loss: 0.004193956255912781\n",
      "9, 20 loss: 0.004215337336063385\n",
      "10, 10 loss: 0.00407411128282547\n",
      "10, 20 loss: 0.004220125675201416\n",
      "training local model\n",
      "1, 10 loss: -0.0\n",
      "2, 10 loss: -0.0\n",
      "3, 10 loss: -0.0\n",
      "4, 10 loss: -0.0\n",
      "5, 10 loss: -0.0\n",
      "6, 10 loss: -0.0\n",
      "7, 10 loss: -0.0\n",
      "8, 10 loss: -0.0\n",
      "9, 10 loss: -0.0\n",
      "10, 10 loss: -0.0\n",
      "training local model\n",
      "1, 10 loss: -0.0\n",
      "2, 10 loss: -0.0\n",
      "3, 10 loss: -0.0\n",
      "4, 10 loss: -0.0\n",
      "5, 10 loss: -0.0\n",
      "6, 10 loss: -0.0\n",
      "7, 10 loss: -0.0\n",
      "8, 10 loss: -0.0\n",
      "9, 10 loss: -0.0\n",
      "10, 10 loss: -0.0\n",
      "training local model\n",
      "training local model\n",
      "1, 10 loss: 0.009280662536621093\n",
      "1, 20 loss: 0.008359681963920593\n",
      "1, 30 loss: 0.007833428978919983\n",
      "1, 40 loss: 0.007362743616104126\n",
      "2, 10 loss: 0.007341563701629639\n",
      "2, 20 loss: 0.007163227796554566\n",
      "2, 30 loss: 0.007160112261772156\n",
      "2, 40 loss: 0.006904221177101135\n",
      "3, 10 loss: 0.006896939873695373\n",
      "3, 20 loss: 0.007057373523712158\n",
      "3, 30 loss: 0.006831178069114685\n",
      "3, 40 loss: 0.00672691822052002\n",
      "4, 10 loss: 0.0069898486137390135\n",
      "4, 20 loss: 0.006729210615158081\n",
      "4, 30 loss: 0.006368790268898011\n",
      "4, 40 loss: 0.006626349687576294\n",
      "5, 10 loss: 0.006582208871841431\n",
      "5, 20 loss: 0.006344024538993836\n",
      "5, 30 loss: 0.006609132289886474\n",
      "5, 40 loss: 0.006501368880271911\n",
      "6, 10 loss: 0.0065254807472229\n",
      "6, 20 loss: 0.00648048996925354\n",
      "6, 30 loss: 0.006602396965026855\n",
      "6, 40 loss: 0.006573116779327393\n",
      "7, 10 loss: 0.006130501627922058\n",
      "7, 20 loss: 0.006492922902107239\n",
      "7, 30 loss: 0.006459241509437561\n",
      "7, 40 loss: 0.006462861895561218\n",
      "8, 10 loss: 0.00616838812828064\n",
      "8, 20 loss: 0.006341644525527954\n",
      "8, 30 loss: 0.006293589472770691\n",
      "8, 40 loss: 0.006381840109825134\n",
      "9, 10 loss: 0.00619510293006897\n",
      "9, 20 loss: 0.006561119556427002\n",
      "9, 30 loss: 0.006026526689529419\n",
      "9, 40 loss: 0.006325989365577698\n",
      "10, 10 loss: 0.006241756677627564\n",
      "10, 20 loss: 0.006290048360824585\n",
      "10, 30 loss: 0.006325953006744385\n",
      "10, 40 loss: 0.006254655718803406\n",
      "trainging global model\n",
      "global index train smaple count: 84\n",
      "finish\n",
      "local model len:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\bspli\\./Bspli\\utils\\Partitioning.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ma = torch.tensor(means)\n",
      "d:\\conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\loss.py:928: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n",
      "d:\\conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\loss.py:928: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    }
   ],
   "source": [
    "mnist_tensor = torch.from_numpy(mnist)\n",
    "print(f'mnist tensor shape: {mnist_tensor.shape}')\n",
    "\n",
    "idx = index.Indexing(\n",
    "    gl_size=10000, \n",
    "    ll_size=3000,\n",
    "    g_epoch_num=5,\n",
    "    l_epoch_num=10,\n",
    "    g_hidden_size=10,\n",
    "    l_hidden_size=20,\n",
    "    g_block_range=4,\n",
    "    l_block_range=4,\n",
    "    random_partitioning=False\n",
    ")\n",
    "idx.train(mnist_tensor)\n",
    "\n",
    "print(f\"local model len:{len(idx._l_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.7\n",
      "pred: tensor([    0, 32248,  8728, 18932, 24149, 42338, 52295, 26251, 50173, 53634,\n",
      "        24330, 54159, 57528,  1482, 18123, 23947, 20034, 52057, 33825, 21654,\n",
      "        31008, 55208, 22477, 44282, 47968, 54203, 19825, 33909, 24708, 34474,\n",
      "        26413, 16017, 46824, 46358,  1516, 34557, 21629, 29021, 10740, 24107,\n",
      "        52665,  1864,  5036, 40546, 22322, 52231, 37284, 21976, 36697, 25675,\n",
      "        54189, 11396, 42555, 52540, 44263, 18404, 25762, 14736,  5210, 59212,\n",
      "        22569,  6772, 21244, 21583, 35838, 59846, 21210, 13502, 52559, 13862,\n",
      "        18172, 41980, 43997, 53812, 18162, 15975, 50187, 28384, 35570, 24271,\n",
      "        21661, 15907, 38896, 15444, 50071, 45567, 26313,  2395, 11936, 16112,\n",
      "        49334, 44476, 18189, 17298, 29023, 26364, 36242, 18197, 38940, 25557],\n",
      "       dtype=torch.int32)\n",
      "isin same block: False\n",
      "CPU times: total: 625 ms\n",
      "Wall time: 100 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def recall(pred, true):\n",
    "    x = np.isin(pred, true)\n",
    "    return x.sum() / true.size\n",
    "\n",
    "qp = torch.from_numpy(mnist[0])\n",
    "# print(qp)\n",
    "pred = idx.query(qp, k=100)\n",
    "print(f\"recall: {recall(pred, FLAT_I)}\")\n",
    "\n",
    "pred = pred.to(torch.int)\n",
    "print(f\"pred: {pred}\")\n",
    "\n",
    "print(f\"isin same block: {idx.isin(qp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.95\n",
      "pred: tensor([    0, 32248,  8728, 18932, 30483, 24149, 42338, 52295, 26251, 50173,\n",
      "        53634, 24330, 54159, 57528,  1482, 53428, 18123, 31379, 52864, 10536,\n",
      "        36087, 30489, 23947, 20034, 52057, 33825, 21654, 31008, 55208, 22477,\n",
      "        44282, 47968, 54203, 19825,  1634, 27378, 33909, 15378, 24708, 34474,\n",
      "        26413, 16017, 46824, 46358,  1516, 34557, 21629, 29021, 10740, 24107,\n",
      "         5688, 52665,  1864,  5036, 39031,  1978, 40546, 22322, 52231, 37284,\n",
      "         5970, 21976, 16945, 36697, 25675, 54189, 11396, 42555, 33445, 52540,\n",
      "        44263, 18404, 24232, 54184, 25762, 14736, 33970,  5210, 59212,  8642,\n",
      "        22569, 15052,  2933,  6772, 22963,  6516,   832, 21244, 21583, 35838,\n",
      "        59846, 21210, 13502, 52559, 13862, 18172, 41980, 43997, 53812, 18162],\n",
      "       dtype=torch.int32)\n",
      "CPU times: total: 656 ms\n",
      "Wall time: 116 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred = idx.query_with_threshold(qp, k=100, threshold=0.02, g_block_range=10)\n",
    "print(f\"recall: {recall(pred, FLAT_I)}\")\n",
    "pred = pred.to(torch.int)\n",
    "print(f\"pred: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "def benchmark_knn_query(data, index, size=1000, k=100):\n",
    "    indices = np.random.choice(data.shape[0], size, replace=False)\n",
    "    query_time = 0\n",
    "    cur_recall = 0\n",
    "    isin_count = 0\n",
    "\n",
    "    # query\n",
    "    for i in indices:\n",
    "        q = torch.from_numpy(data[i])\n",
    "        start = time.time()\n",
    "        qk = index.query(q, k=100, g_block_range=7, l_block_range=4)\n",
    "        query_time += (time.time() - start)\n",
    "        D, FLAT_I = flat.search(data[i].reshape(1, data.shape[1]), k=k) \n",
    "        cur_recall += recall(qk, FLAT_I)\n",
    "        # isin_count += 1 if index.isin(q) else 0\n",
    "        \n",
    "    result.append((query_time/1000, cur_recall/1000, isin_count/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# recall(pred, FLAT_I)\n",
    "\n",
    "benchmark_knn_query(mnist, idx, size=1000, k=100)\n",
    "print(result)\n",
    "print(idx.get_search_blocks_num())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "def benchmark_knn_query_with_threshold(data, index, size=1000, k=100):\n",
    "    indices = np.random.choice(data.shape[0], size, replace=False)\n",
    "    query_time = 0\n",
    "    cur_recall = 0\n",
    "    isin_count = 0\n",
    "\n",
    "    # query\n",
    "    for i in indices:\n",
    "        q = torch.from_numpy(data[i])\n",
    "        start = time.time()\n",
    "        qk = idx.query_with_threshold(q, k=100, threshold=0.05, g_block_range=12)\n",
    "        query_time += (time.time() - start)\n",
    "        D, FLAT_I = flat.search(data[i].reshape(1, data.shape[1]), k=k) \n",
    "        cur_recall += recall(qk, FLAT_I)\n",
    "        # isin_count += 1 if index.isin(q) else 0\n",
    "        \n",
    "    result.append((query_time/1000, cur_recall/1000, isin_count/1000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "benchmark_knn_query_with_threshold(mnist, idx, size=1000, k=100)\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
